\section{Introduction}

%%\begin{itemize}
%%    \item Complex large-scale agent-based models are becoming more common, in several application areas.
%%    \item These models are data-driven and specific, customized to answer specific questions or model specific phenomena.
%%    \item Often, we see multiple models for the same general phenomenon, such as epidemic spread, technology adoption, disaster evacuation, etc., created with different parameters, data sources, and model structure.
%%    \item This raises the general question of how to compare such models~\cite{axtell96aligning,burton99validation}.
%%    \item For example, in the case of the adoption of rooftop solar panels by households in two different parts of the country, we might wish to know whether it is somehow easier to have a large number of adoptions in one region than another, or whether it is just a chance difference.
%%    \item In this paper, we present a general framework to make these types of model comparisons. Then we present a specific example of the application of this framework to the comparison of two different models of the adoption of rooftop solar panels by households in two different regions of the United States.
%%    \item The rest of this paper is organized as follows. First we present the general methodological framework. Then we describe the two models we are comparing. In this specific case, model comparison requires learning a phase transition boundary in a contagion model. However, since the agent-based simulations are expensive (time-consuming) to run, we develop an active learning method with the simulation in the loop. After describing this method in detail, we present results from computational experiments with the two models. We end with a discussion of related work and future directions.
%%\end{itemize}

Complex large-scale agent-based models (ABM) are becoming increasingly common, in a huge number of application areas,
such as public health, infrastructure systems such as transportation and power, disaster evacuation, and
technology adoption.
ABMs are designed to answer specific questions within an application, and their design is data-driven.
As a result, there can be multiple ABMs with a similar overall structure, but differences in the
specific model components, their interactions and parameters.
This raises the general question of how to compare such models~\cite{axtell96aligning,burton99validation}.
Axtell et al.~\cite{axtell96aligning} were the first to address this question, and developed the ``docking'' technique,
which involves verifying whether or not the dynamical properties of one ABM can be regenerated by another.
However, this is computationally intractable as ABMs become complex, and a very restrictive notion.
Other notions of validation and equivalence of ABMs have also been explored, e.g., \cite{Volk-Makarewicz:2017:MVA:3242181.3242289,Bharathy:2010:VAB:2433508.2433559}. However, these only allow restricted forms of comparison between ABMs, and do not provide efficient computational tools for comparison based on specific characteristics in the phase space.

At an abstract level, these questions are analogous to the many notions of phase space equivalence of dynamical systems \cite{mortveit, adiga:wsc19}. The approach of~\cite{axtell96aligning} attempts to compare precise structural properties in the phase space, which is NP-hard, in general~\cite{adiga:wsc19}.  
In this paper, we present a general and more scalable framework to make these types of comparisons between ABMs, based on comparing approximate representations of the phase spaces of the ABMs; specifically, we consider the structure of the region which correspond to ``phase transitions'', i.e., where the system has very different behavior by small change in parameters. 
While this does not correspond to exact phase space equivalence, this notion can give useful insights in many applications where the ABMs work on different domains.

As a specific example, we consider ABMs for 
 adoption of rooftop solar panels by households in two different regions of the United States.
A question of interest for local governments and utilities is to
understand who will adopt solar, and how to increase adoption.
We compare two different ABMs, one developed for California \cite{zhang16solar},
and the other for Virginia~\cite{hu19rooftop}. The probability of adoption by a household depends on a number of factors,
including demographics and characteristics of the house, as well as \emph{peer effects}, captured by the number of
households who have adopted within a 1-4 mile range. The two models have a large fraction of common factors,
but some which are distinct, e.g., pool ownership, which is a factor in the ABM of~\cite{zhang16solar},
but not in that of~\cite{hu19rooftop}. The datasets used in the calibration of these two ABMs have different
characteristics, with a much larger adoption rate in California.
In order to compare these two ABMs, we study whether it is somehow easier to have a large number of adoptions 
in one region than another, or whether it is just a chance difference. 
Our contributions are summarized below.
\begin{itemize}
\item
We design a general methodological framework based on the response surface method. We introduce a notion of
\emph{characteristic distribution} of an ABM in terms of phase space properties as the probability distribution of the ABM outcomes within some range. A specific example we consider is a range within which the dynamics exhibits a phase transition. We quantify the \emph{disagreement} between two ABMs through the differences in these probability distributions.
\item 
We design a machine learning approach based on active learning for estimating the characteristic distribution.
Active learning helps us reduce the number of times the simulation has to be run.
Thus, this is a much
more scalable approach for complex ABMs.
\item
We illustrate our approach on the two ABMs for comparing solar adoption.
\textcolor{red}{Our results:}
\end{itemize}

\noindent
\textbf{Organization.}
The rest of this paper is organized as follows. First we present the general methodological framework
(Section \ref{sec:framework}).
Then we describe the two models we are comparing (Section \ref{section:rooftop}).
After describing this method in detail (Section \ref{sec:learning}), we present results from computational experiments with the two models (Section \ref{sec:experiments}). We end with a discussion of related work and future directions.
