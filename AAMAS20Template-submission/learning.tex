\section{Model Comparison Method}
\label{sec:learning}

% {\color{magenta} Use the reference of framework section to write this scetion more formally}\\

In this section, we instantiate the framework described in Section~\ref{sec:framework} to compare the models described in Section~\ref{section:rooftop}. 
Both the Virginia model and the San Diego model are network contagion models,
where a contagion (in this case a technology: rooftop solar panels) spreads through a network. Both models belong to the general class of $SI$ contagion models,
drawn from mathematical epidemiology, where $S$ stands for $Susceptible$
and $I$ stands for $Infectious$. In our case, $Infectious$ corresponds to adopter. Once a household has adopted solar panels, their peer influence on their neighbors is assumed to persist indefinitely.
This means that, in the limit, all households in both models will be adopters, as
long as the parameters are set in such a way that the probability of adoption if at
least one neighbor has adopted is non-zero. However, depending on the parameter
settings, it can be the case that the probabilities of adoption are so low that 
we see very few adoptions in the duration for which the simulations are run. Network
structure can also play a role in speeding up or slowing down the spread of the 
contagion through the network.

As the probability of adoption increases, the SI model undergoes a phase transition,
where the simulation shows a sharp qualitative change in its behavior. As the probability of adoption crosses a threshold, the simulation quickly changes from only a few nodes being adopters to a lot (or most) of the nodes becoming adopters in a short amount of time. Due to this qualitative behavior, we choose just two bins to describe each simulation in Section~\ref{section:rooftop}, which we refer to as $B_0$ an corresponding to small and large numbers of adopters. The actual values chosen are listed in Section~\ref{sec:experiments}.

Another nice aspect of this scenario is that the variance in the output of the 
simulation shows a sharp peak at the phase transition boundary, and tends to be low
away from the boundary. This is illustrated in Figure~\ref{fig:diagDisplay}. Thus, by running multiple runs of the simulation for any 
chosen point in parameter space, we get a clear signal if the point is close to the phase transition boundary. We make use of this fact in our active learning algorithm below.

In this section, we introduce an active learning~\cite{sung95active} method for learning the decision boundary. 
Figure \ref{fig:workflow} gives a general idea of the proposed methodology. The goal of the proposed method is
to learn the decision boundary that separates large breakout from small breakouts with minimum number of queries in the parameter space.

Notations used in the remainder of the paper for method description, results and discussion:\\
$start$, $end$ = the endpoints of a line in the 2D parameter search space on which binary search is performed. These points qualify as end points only if they can separate the search space into small and large outbreaks. \\
\noindent
$pMean$, $pStdev$ = mean and standard deviation of the number of adoptions across 20 replicates of the diffusion model at the last time step. In this instance, the diffusion model (or ABM) is evaluated at $p$.\\
\noindent
$evaluatedPoints$ = List of labeled points in the 2D parameter search space. A point is labeled as 0 if the mean number of adoptions is less than a certain threshold.\\
\noindent
$boundaryPoints$ = List of points in the 2D parameter search space that are close to or on the decision boundary. If the standard deviation of the number of adoptions at the point is greater than a certain threshold, then the point is classified as a boundary point.

Initially, a suitable range of the parameter search space is selected. In our experiments, the search space is instantiated with two parameters. 
A seed labeled dataset is created from the search space to train the classifier the first time. 
Binary search is performed across the diagonal of the parameter space to find a potential boundary point, followed by querying neighboring points of such a boundary point with uncertainty sampling.
% TODO: Roughly 5-10 points are labeled -> this means how much % of data is labeled for first training of the classifier. 
% Why we look for variacne of the point - describe that in 1 sentence
The classifier is then trained on this seed dataset.

Further iterations generate more labeled data by intelligently querying unlabeled instances in the parameter search space by employing uncertainty sampling technique.
To find potential boundary points, the algorithm exploits the knowledge gained in terms of the decision boundary, $evaluatedPoints$, and $boundaryPoints$ in the parameter space. To choose the next set of endpoints to perform binary search, a point $start$, is chosen such that it is close to or on the current decision boundary, but far away from existing $boundaryPoints$ and $evaluatedPoints$ in the search space. 
Evaluate the ABM outcome at $start$ in terms of $sMean$ and $sStdev$. The $end$ point is chosen from $evaluatedPoints$ such that, it is the farthest point from $start$ and with an opposite label of the $start$ point. Points are queried in the search space until the termination condition is reached.
% Roughly how much % of points is queried at the end as compared to the area of the search space

%Once the point is queried, mean and standard deviation are used to evaluate the role of the queried point. If a point is found to be potentially very close to/on the boundary, then, the classifier is trained on the incremental set of labeled points. The knowledge of the latest decision boundary also influences the choice of unlabeled point tp be queried.




\begin{algorithm}

\caption{Active learning for predicting decision boundary}
\label{alg:methodAlgo}
\begin{algorithmic}[1]

\State $Input$ : $DiffusionModel$,
2D search space for parameters $p1,p2$ 
\State $Output$ : $evaluatedPoints$ , $boundaryPoints$

\Procedure{LearnDecisionBoundary}{}
    \State $start$  = $[p1Min, p2Min]$
    \State $end$  = $[p1Max, p2Max]$
    \State $[boundaryPt, bMean, bStdev] = \Call{BinarySearch}{start,end}$
    \State \Call{EvaluateNearbyPoints}{$boundaryPt$} Add these points to $evaluatedPoints$
    \State $noRounds$ = 1
    \While{$noRounds < = 5$}
        \State \textbf{\textit{$nextPt$}} = \Call{GetNextPointViaActiveLearning}{}
        \State $[nextPt, nMean, nStdev]$ = \Call{RunDiffusionModel}{$nextPt$}
        \If{$nStdev >= STDEV-THRESHOLD$ }
            \State Add $nextPt$ to $boundaryPoints$
        \Else 
            \State Add $[nextPt, nMean, nLabel]$ to $evaluatedPoints$
        \EndIf
        \State $oppositePt$ = Another point with label opposite to $nLabel$ and farthest from $nextPt$
        \State $[boundaryPt, bMean, bStdev] = \Call{BinarySearch}{nextPt,oppositePt}$
        \State \Call{EvaluateNearbyPoints}{$boundaryPt$} Add these points to $evaluatedPoints$
        
        \State $noRounds++$
        
     \EndWhile
    
    \State return \textbf{$evaluatedPoints$ , $boundaryPoints$}

\EndProcedure
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H] {
\caption{Binary search to find a potential boundary point}
\label{alg:binSearch}
\begin{algorithmic}[1]
\Procedure{BinarySearch}{$pt_1, pt_2$}
    \State $[pt_1Mean, pt_1Stdev]$ = \Call{RunDiffusionModel}{$pt_1$}
    \If{$pt_1Stdev >= STDEV-THRESHOLD$ }
            \State Add $pt_1$ to $boundaryPoints$
            \State return \textbf{[$pt_1, pt_1Mean, pt_1Stdev$]}
        \Else 
            \State Add $[pt_1,pt_1Mean,pt_1Label]$ to $evaluatedPoints$
    \EndIf
    \State $[pt_2Mean, pt_2Stdev]$ = \Call{RunDiffusionModel}{$pt_2$}
    \If{$pt_2Stdev >= STDEV-THRESHOLD$ }
            \State Add $pt_2$ to $boundaryPoints$
            \State return \textbf{[$pt_2, pt_2Mean, pt_2Stdev$]}
        \Else 
            \State Add $[pt_2,pt_2Mean,pt_2Label]$ to $evaluatedPoints$
    \EndIf
    \State $m = (pt_1 + pt_2)/2.0$ 
    \While{ $m \notin boundaryPoints$ and $pt_1Label \neq pt_2Label$}
        \State $[mMean, mStdev]$ = \Call{RunDiffusionModel}{$m$}
        \If{$mStdev >= STDEV-THRESHOLD$ }
            \State Add $m$ to $boundaryPoints$
            \State return \textbf{[$m, mMean, mStdev$]}
        \Else 
            \State Add $[m,mMean,mLabel]$ to $evaluatedPoints$ 
        \EndIf
        \State Assign $m$ to $pt_1$ or $pt_2$, s.t. $pt_1$ and $pt_2$ have different labels
        \State $m = (pt_1 + pt_2)/2.0$ 
    \EndWhile
\EndProcedure
\end{algorithmic}
}
\end{algorithm}